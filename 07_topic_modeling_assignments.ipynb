{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LDA experimentations:\n",
    "------------------------\n",
    "1. Take your favorite dataset from the previous demonstrations and read a sample from it in Python.\n",
    "2. Form a bag-of-words representation for each document. At first, do not use any preprocessing for your data.\n",
    "3. Now, train an LDA model with these documents. Start with very tiny dataset and a small number of topics as the training might take a long time on the virtual machine.\n",
    "\n",
    "4. Once you have the model trained, print the most common words from each topic to see whether the topics make any sense. Then start increasing your data and topic amount.\n",
    "\n",
    "5. Try preprocessing your input data by e.g. removing stop words. Do the results improve?\n",
    "\n",
    "6. Once you are happy with the topics, select a document and transform it into topic distributions. Do they make sense?\n",
    "\n",
    "7. As the last part, transform all your documents into topic distribution representations and search for the most similar documents for your example document (based on cosine similarity).\n",
    "\n",
    "Report the observation you made during these experiments. If the task seems too difficult feel free to use the code snippets provided in the lecture notes, all of the assignments should be doable by simply copying the code from the notes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
